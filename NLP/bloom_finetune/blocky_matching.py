# -*- coding: utf-8 -*-
"""Blocky_Matching.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fY_J8LdTRXhPGJ9CLnjuYlUCTQ8hcCPb
"""

# Install required libraries
# !pip install -q bitsandbytes datasets accelerate loralib
# !pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git

# Load in the bloom model
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import torch
import torch.nn as nn
import bitsandbytes as bnb
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    'bigscience/bloom-3b',
    load_in_8bit = True,
    device_map = 'auto',
)

tokenizer = AutoTokenizer.from_pretrained('bigscience/Tokenizer')

# Post processing the model
for param in model.parameters():
  param.requires_grad = False # freeze the model - train adapters later
  if param.ndim == 1:
    # cast the small parameters (e.g. layernorm) to fp32 for stability
    param.data = param.data.to(torch.float32)

model.gradient_checkpointing_enable() # reduce number of stored activations
model.enable_input_require_grads()

class CastOutputToFloat(nn.Sequential):
  def forward(self, x): return super().forward(x).to(torch.float32)
model.lm_head = CastOutputToFloat(model.lm_head)

#define function to print all the trainable parameters in the model
def print_trainable_parameters(model):
  trainable_params = 0
  all_params = 0
  for _, param in model.named_parameters():
    all_params += param.numel()
    if param.requires_grad:
      trainable_params += param.numel()
  print(
      f"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params / all_params}"
  )

# Use LoRA, low-rank adapters, in order to target and define the parameters that should be optimized when we fine-tune
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r = 32,
    lora_alpha = 48,
    target_modules = ['query_key_value', 'attention'],
    lora_dropout = 0.1,
    bias = 'none',
    task_type = 'CAUSAL_LM'
)

model = get_peft_model(model, config)
print_trainable_parameters(model)

# load in transformers and our dataset
import transformers
from datasets import load_dataset

dataset_name = 'alexplash/blockymatching'
input = 'INPUT'
sub = 'SUBCATEGORIES'

dataset = load_dataset(dataset_name)
print(dataset)
print(dataset['train'][0])

# define format of data being used to train the model
def generate_prompt(input: str, sub: str):
  prompt = f"### DESCRIPTION OF WORK/DATA: \n'{input}'\n\n### KEYWORDS: \n'{sub}' ### END"
  return prompt

dataset = dataset.map(lambda samples: tokenizer(generate_prompt(samples['INPUT'], samples['SUBCATEGORIES'])))

# train the model
trainer = transformers.Trainer(
    model = model,
    train_dataset = dataset['train'],
    args = transformers.TrainingArguments(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 4,
        warmup_steps = 100,
        max_steps = 20,
        learning_rate = 1e-3,
        fp16 = True,
        logging_steps = 1,
        output_dir = 'outputs'
    ),
    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm = False)
)

model.config.use_cache = False,
trainer.train()

# uploading model to Hugging Face
HUGGING_FACE_USER_NAME = 'alexplash'

from huggingface_hub import notebook_login
notebook_login()

model.push_to_hub(f"{HUGGING_FACE_USER_NAME}/Blocky_Matching", use_auth_token = True)

# load the model back into this session
import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = f"{HUGGING_FACE_USER_NAME}/Blocky_Matching"
config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict = True, load_in_8bit = True, device_map = 'cuda')
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

# load the LoRA model
model = PeftModel.from_pretrained(model, peft_model_id)

"""**Try out the model**"""

from IPython.display import display, Markdown

def categorize(input: str):
  batch = tokenizer(f"### DESCRIPTION OF WORK/DATA: \n'{input}'\n\n### KEYWORDS: \n", return_tensors = 'pt')

  with torch.cuda.amp.autocast():
    output_tokens = model.generate(**batch, max_new_tokens = 50)

  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens = True))))

input = "Seeking in-depth research papers and project submissions on the resurgence of hand-lettering in digital media advertising, focusing on its psychological impact on consumer behavior and brand recall."
categorize(input)